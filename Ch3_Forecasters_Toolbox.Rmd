---
title: "Ch3 | The Forecasters Toolbox"
author: "Michael Rose"
date: "November 3, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fpp2)
```

# Chapter 3 | The Forecaster's Toolbox

In this chapter we will discuss: 

* General tools useful for different forecasting situations 
* benchmark forecasting methods 
* transformations to make forecasting simpler 
* methods for checking whether a forecasting method has utilized the available information 
* techniques for computing prediction intervals 

# 3.1 | Some simple forecasting methods 

We will use the following four forecasting methods as benchmarks througout this book: 

### Average Method

All the forecasts of all future values are equal to the average of the historical data. Let the historical data be $y_1, ..., y_T$. Then 
\begin{center}
$y_{T+h | T} = \bar{y} = (y_1 + ... + y_T) / T$
where $y_{T+h|T}$ is the estimate of $y_{t+h}$ based on the data $y_1, ..., y_T$
\end{center}

```{r}
# y contains the time series
# h is the forecast horizon

# mean(y, h)
```

### Naive Method 

We can simply set all forecasts to be the value of the last observation. 

\begin{center}
$y_{T + h|T} = y_T$
\end{center}

This method works well for many economic and financial time series 

```{r}
# naive(y, h)
# rwf(y, h)  equivalent alternative
```


Because a naive forecast is optimal when data follow a random walk, these are also called **Random Walk Forecasts**. 

### Seasonal Naive Method 

When our data is highly seasonal, we can set our forecast to be equal to the last observed value from the same season of the year. 

\begin{center}
$y_{T+h|T} = y_{T+h-m(k+1)}$
where $m$ is the seasonal period 
$k$ is the integer part of $\frac{h-1}{m}$ (the number of complete years in the forecast period prior to time $T+h$)
\end{center}

```{r}
# snaive(y, h)
```


### Drift Method 

A variation on the naive method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the **drift**) is set to be the average change seen in the historical data. 

\begin{center}
$y_{T+h|T} = y_T + \frac{h}{T-1} \sum_{t = 2}^T (y_t - y_{t-1}) = y_T + h(\frac{y_T - y_1}{T - 1})$
\end{center}

This is equivalent to drawing a line between the first and last observations, and extrapolating it into the future. 

```{r}
# rwf(y, h, drift = TRUE)
```

### Examples 

```{r}
# set training data from 1992 to 2007 
beer2 <- window(ausbeer, start = 1992, end = c(2007, 4))

# plot forecasts for first 3 methods
autoplot(beer2) + 
  autolayer(meanf(beer2, h = 11), series = "Mean", PI = FALSE) + 
  autolayer(naive(beer2, h=11), series = "Naive", PI=FALSE) + 
  autolayer(snaive(beer2, h=11), series="Seasonal Naive", PI=FALSE) + 
  ggtitle("Forecasts for Quarterly Beer Production") + 
  xlab("Year") + ylab("Megalitres") + 
  guides(colour = guide_legend(title = "Forecast"))
```

Here we apply the non-seasonal methods to a series of 200 days of Google daily closing stock price: 

```{r}
autoplot(goog200) + 
  autolayer(meanf(goog200, h = 40), series = "Mean", PI=FALSE) + 
  autolayer(rwf(goog200, h = 40), series = "Naive", PI=FALSE) + 
  autolayer(rwf(goog200, h = 40, drift = TRUE), series = "Drift", PI=FALSE) + 
  ggtitle("Google Stock (Daily Ending 6DEC13)") + 
  xlab("Day") + ylab("Closing Price (US$)") + 
  guides(colour = guide_legend(title = "Forecast"))
```

These methods are generally considered benchmarks, but they may be the most effective in some cases. When we develop a new method, they must be better than these benchmarks -- else they aren't worth considering. 

# 3.2 | Transformations and Adjustments 

In this section we deal with 4 kinds of adjustments: 

* Calendar Adjustments 
* Population Adjustments 
* Inflation Adjustments 
* Mathematical Transformations 

We want to simplify the patterns in the historical data by removing known sources of variation or by making the pattern more consistent across the whole data set. 

### Calendar Adjustments 

Some of the variation in seasonal data may be due to simple calendar effects. In these cases, it is easy to remove the variation before fitting a forecasting model. 

For example, if we are studying monthly milk production on a farm, there will be variation between the months simply because the different number of days in each month (in addition to the seasonal variation across the year). 

```{r}
# monthdays() will compute the number of days in each month or quarter
dframe <- cbind(Monthly = milk, DailyAverage = milk / monthdays(milk)) 

autoplot(dframe, facet=TRUE) + 
  xlab("Years") + ylab("Pounds") + 
  ggtitle("Milk Production per Cow")
```

By looking at the average daily production instead of the average monthly production, we effectively remove the variation due to different month lengths. 

### Population Adjustments 

Any data affected by population changes can be adjusted to give per-capita data. We can consider the data per person, or thousand people, or million people. 

### Inflation Adjustments 

Data which are affected by the value of money are continuously changed by the value of that currency. For example, $200,000 today is worth less than the same amount in the year 2000. Thus we can standardize our monetary measurements in year X dollars. 

To make these adjustments, a price index is used. If $z_t$ denotes the price index and $y_t$ denotes the original house price in year $t$, then $x_t = \frac{y_t}{z_t} * z_{2000}$ gives the adjusted price at year 2000 dollar values. 

### Mathematical Transformations

If the data shows variation that increases or decreases with the level of the series, then a transformation can be useful. A log transform is often useful. 

Other transforms include square roots and cube roots. These are called **power transformations** and can be expressed in the form $w_t = y_t^p$. 

Another useful family of transformations that includes both logs and power transformations is the family of **Box-Cox Transformations**. 

$w_t = \log(y_t)$ if $\lambda = 0$. $w_t = \frac{y_t^{\lambda} - 1}{\lambda}$ otherwise. 

A good value of $\lambda$ is one which makes the size of the seasonal variation about the same across the whole series. 

```{r}
# the BoxCox.lambda() function will choose a value of lambda for you
(lambda <- BoxCox.lambda(elec))

# plot 
autoplot(BoxCox(elec, lambda))
```

Having chosen a transformation, we need to forecast the transformed data. Then, we need to reverse the transformation (or back-transform) to obtain forecasts on the original scale. 

The Reverse-BoxCox transformation is given by 

$y_t = \exp(w_t)$ if $\lambda = 0$
$y_t = (\lambda w_t + 1)^{\frac{1}{\lambda}}$ otherwise. 

### Features of Power Transformations 

* If some $y_t \leq 0$, no power transformation is possible unless all observations are adjusted by adding a constant to all values. 
* Choose a simple value for $\lambda$. It makes explanations easier. 
* The forecasting results are relatively insensitive to the value of $\lambda$
* Often no transformation is needed 
* Transformations sometimes make little difference to the forecasts but have a large effect on prediction intervals

### Bias Adjustments 

One issue with using mathematical transformations like BoxCox is that the back-transformed point forecast will not be the mean, but instead the median, of the forecast distribution. 

If we need the mean, for example if we wish to add up sales forecasts from various regions to form a forecast for the whole country, the back-transformed mean is given by: 

$y_t = \exp(w_t) [1 + \frac{\sigma_h^2}{2}]$ if $\lambda = 0$
$y_t = (\lambda w_t + 1)^{\frac{1}{\lambda}}[1 + \frac{\sigma_h^2(1 - \lambda)}{2(\lambda w_t + 1)^2}]$ otherwise. 

Where $\sigma_h^2$ is the h-step forecast variance. The larger the forecast variance, the bigger the difference between mean and the median. 

The difference between the back-transform without the adjustment and the back transform with the adjustment is called the **bias**. When we use the mean rather than the median, we say our point forecasts have been **bias-adjusted**

To consider how much the bias adjustment changes things: 

```{r}
fc <- rwf(eggs, drift = TRUE, lambda = 0, h = 50, level = 80)
fc2 <- rwf(eggs, drift = TRUE, lambda = 0, h = 50, level = 80, biasadj = TRUE)

autoplot(eggs) + 
  autolayer(fc, series = "Simple back-transformation") + 
  autolayer(fc2, series = "Bias Adjusted", PI = FALSE) + 
  guides(color = guide_legend(title = "Forecast"))
```

The blue in the figure above shows the forecast medians while the red line shows the forecast means. Notice how the skewed forecast distribution pulls up the point forecast when we use the bias adjustment. 

Bias adjustment is not done by default in the forecast package. We must use the arg biasadj=TRUE when we select our boxcox transformation parameter. 

# 3.3 | Residual Diagnostics 
