* Recurrent Neural Networks for Time Series: Sunspots
:PROPERTIES:
:header-args: :session R-session :results output value table :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src 

#+RESULTS: round-tbl


#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(magrittr)
library(glue)
library(forcats)
library(lubridate)

# time series 
library(timetk)
library(tidyquant)
library(tibbletime)

# visualization 
library(cowplot)

# preprocessing
library(recipes)

# sampling / accuracy 
library(rsample)
library(yardstick)

# modeling
library(keras)
library(tfruns)
#+END_SRC

** Data

Our dataset is a ts class (not tidy), so we need to convert it to a tidy data set first using the tk_tbl function. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
sun_spots <- datasets::sunspot.month %>%
    tk_tbl() %>%
    mutate(index = as_date(index)) %>%
    as_tbl_time(index = index)

sun_spots %>%
    head()
#+END_SRC

#+RESULTS:
|      index | value |
|------------+-------|
| 1749-01-01 |  58.0 |
| 1749-02-01 |  62.6 |
| 1749-03-01 |  70.0 |
| 1749-04-01 |  55.7 |
| 1749-05-01 |  85.0 |
| 1749-06-01 |  83.5 |

** Exploratory Data Analysis 

#+BEGIN_SRC R :file plot.svg :results graphics file
sun_spots %>%
    ggplot(aes(x = index, y = value)) +
    geom_point(color = palette_light()[[1]],
               alpha = 0.5) +
    theme_tq() +
    labs(title = "From 1749 to 2013 (Full Data)") -> p1

sun_spots %>%
    filter_time("start" ~ "1800") %>% 
    ggplot(aes(x = index, y = value)) +
    geom_line(color = palette_light()[[1]],
              alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = "loess", span = 0.2, se = FALSE) +
    theme_tq() +
    labs(title = "From 1749 to 1759 (Zoomed In To Show Cycle)",
         caption = "datasets::sunspot.month") -> p2

ggdraw() +
    draw_label("Sunspots",
               size = 18,
               fontface = "bold",
               color = palette_light()[[1]]) -> p_title

plot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Backtesting: Time Series Cross Validation

The backtesting strategy is this:

- use 100 years for the training set
- use 50 years for the validation set
- select a skip span of about 22 years to approximately evenly distribute the samples into 6 sets that span the entire 265 year history of the sunspots data
- set cumulative to FALSE to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
periods_train <- 12 * 100
periods_test <- 12 * 50
skip_span <- 12 * 22 - 1

(rolling_origin_resamples <- rolling_origin(
    sun_spots,
    initial = periods_train,
    assess = periods_test,
    cumulative = FALSE,
    skip = skip_span))
#+END_SRC

#+RESULTS:
: nil

** Visualizing the Backtesting Strategy

We can use two custom functions: 

- plot_split() plots one of the resampling splits using ggplot
- plot_sampling_plan() scales the plot_split() function to all of the samples using purrr and cowplot 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
plot_split <- function(split,
                       expand_y_axis = TRUE, alpha = 1, size = 1, base_size = 14) {
    # manipulate data
    train_tbl <- training(split) %>%
        add_column(key = "training")

    test_tbl <- testing(split) %>%
        add_column(key = "testing")

    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%
        as_tbl_time(index = index) %>%
        mutate(key = fct_relevel(key, "training", "testing"))

    # collect attributes 
    train_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()

    test_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()

    # visualize 
    gg <- data_manipulated %>%
        ggplot(aes(x = index, y = value, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(title = glue("Split: {split$id}"),
             subtitle = glue("{train_time_summary$start} to {test_time_summary$end}"),
             x = "", y = "") +
        theme(legend.position = "none")

    if (expand_y_axis) {
        sun_spots_time_summary <- sun_spots %>%
            tk_index() %>%
            tk_get_timeseries_summary()

        gg <- gg +
            scale_x_date(limits = c(sun_spots_time_summary$start,
                                    sun_spots_time_summary$end))
    }

    gg
}
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
rolling_origin_resamples$splits[[1]] %>%
    plot_split(expand_y_axis = TRUE) +
    theme(legend.position = "bottom")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
plot_sampling_plan <- function(sampling_tbl,
                               expand_y_axis = TRUE, ncol = 3, alpha = 1, size = 1, base_size = 14,
                               title = "Sampling Plan") {
    # map plot_split() to sampling_tbl 
    sampling_tbl %>%
        mutate(gg_plots = map(splits,
                              plot_split,
                              expand_y_axis = expand_y_axis,
                              alpha = alpha,
                              base_size = base_size)) -> with_plots

    # make combined plot with cowplot 
    plot_list <- with_plots$gg_plots

    p_temp <- plot_list[[1]] +
        theme(legend.position = "bottom")

    legend <- get_legend(p_temp)

    p_body <- plot_grid(plotlist = plot_list,
                        ncol = ncol)

    p_title <- ggdraw() +
        draw_label(title, size = 14, fontface = "bold",
                   color = palette_light()[[1]])

    plot_grid(p_title, p_body, legend,
              ncol = 1, rel_heights = c(0.05, 1, 0.05))
}
#+END_SRC


#+BEGIN_SRC R :file plot.svg :results graphics file
# we could also set expand_y_axis = F to see all the plots zoomed in
rolling_origin_resamples %>%
    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10,
                       title = "Backtesting Strategy: Rolling Origin Sampling Plan")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** The LSTM Model 

To begin, we develop the model on a single sample from the backtesting strategy. We can then apply the model to all samples to investigate performance. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
example_split <- rolling_origin_resamples$splits[[6]]
example_split_id <- rolling_origin_resamples$id[[6]]
#+END_SRC

We can reuse the plot_split function to visualize this split. 

#+BEGIN_SRC R :file plot.svg :results graphics file
example_split %>%
    plot_split(expand_y_axis = FALSE, size = 0.5) +
    theme(legend.position = "bottom") +
    ggtitle(glue("Split: {example_split_id}"))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Data Setup

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
df_trn <- analysis(example_split)[1:800, , drop = FALSE]
df_val <- analysis(example_split)[801:1200, , drop = FALSE]
df_tst <- assessment(example_split)
#+END_SRC

First we combine the training and testing data sets into a single data set with a column that specifies where they came from. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(df <- bind_rows(df_trn %>% add_column(key = "training"),
                df_val %>% add_column(key = "validation"),
                df_tst %>% add_column(key = "testing")) %>%
    as_tbl_time(index = index))    
#+END_SRC

#+RESULTS:
: nil

*** Preprocessing with recipes 

The LSTM algorithm works better when the input data has been centered and scaled. We are also using the step_sqrt to reduce variance and remove outliers. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
rec_obj <- recipe(value ~ ., df) %>%
    step_sqrt(value) %>%
    step_center(value) %>%
    step_scale(value) %>%
    prep()

(df_processed_tbl <- bake(rec_obj, df))
#+END_SRC

#+RESULTS:
: nil

Now we should capture the original center and scale so we can invert the steps after modeling. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
center_history <- rec_obj$steps[[2]]$means["value"]
scale_history <- rec_obj$steps[[3]]$sds["value"]

c("center" = center_history,
  "scale" = scale_history)
#+END_SRC

*** Reshaping the Data 

The input for keras has to be a 3d array of size num_samples, num_timesteps, and num_features. 

- num_samples is the number of observations in the set. This is fed in with batches
- num_timesteps is the length of the hidden state we are talking about above
- num_features is the number of predictors we are using 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# these will be superceded by flags below
n_timesteps <- 12
n_predictions <- n_timesteps
batch_size <- 10

# functions to reshape our data for the time series
build_matrix <- function(tseries, overall_timesteps) {
    t(sapply(1:(length(tseries) - overall_timesteps + 1),
             function (x) tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d <- function(x) {
    dim(x) <- c(dim(x)[1],
                dim(x)[2],
                1)
    x
}

# extract values from data frame 
extract_values <- function(data, key_in) {
    data %>%
        filter(key == key_in) %>%
        select(value) %>%
        pull()
}

train_vals <- df_processed_tbl %>% extract_values("training")
valid_vals <- df_processed_tbl %>% extract_values("validation")
test_vals <- df_processed_tbl %>% extract_values("testing")

# build the windowed matrices 
train_matrix <- build_matrix(train_vals, overall_timesteps = (n_timesteps + n_predictions))
valid_matrix <- build_matrix(valid_vals, overall_timesteps = (n_timesteps + n_predictions))
test_matrix <- build_matrix(test_vals, overall_timesteps = (n_timesteps + n_predictions))

# separate matrices into training and testing parts 
sep_matrices <- function(data, start, end) {
    data %>% .[, start:end] %>% .[1:(nrow(.) %/% batch_size * batch_size), ] %>% reshape_X_3d()
}

x_train <- train_matrix %>% sep_matrices(1, n_timesteps)
y_train <- train_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)

x_valid <- valid_matrix %>% sep_matrices(1, n_timesteps)
y_valid <- valid_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)

x_test <- test_matrix %>% sep_matrices(1, n_timesteps)
y_test <- test_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)
#+END_SRC

*** Building the LSTM model 

Instead of hard coding the hyperparameters, we'll use tfruns to set up an environment where we could easily perform grid search 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
FLAGS <- flags(
    # not a stateful lstm 
    flag_boolean("stateful", FALSE),
    # several layers of LSTM?
    flag_boolean("stack_layers", FALSE),
    # number of samples per batch
    flag_integer("batch_size", 10),
    # size of the hidden state (size of predictions for time series)
    flag_integer("n_timesteps", 12),
    # how many epochs to run for 
    flag_integer("epochs", 100),
    # fraction of units to drop for transformation of inputs
    flag_numeric("dropout", 0.2),
    # fraction of units to drop for the linear transformation of the recurrent state
    flag_numeric("recurrent_dropout", 0.2),
    # loss function
    flag_string("loss", "logcosh"),
    # optimizer 
    flag_string("optimizer_type", "sgd"),
    # size of the LSTM layer 
    flag_integer("n_units", 128),
    # learning rate 
    flag_numeric("lr", 0.003),
    # momentum 
    flag_numeric("momentum", 0.9),
    # early stopping callback
    flag_integer("patience", 10)
)

# number of predictions made is equal to the size of the hidden state 
n_predictions <- FLAGS$n_timesteps

# number of features is the number of predictors 
n_features <- 1

# if we wanted to try different optimizers, we could do that here 
optimizer <- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr,
                                        momentum = FLAGS$momentum))

# callbacks to be passed to the fit function
callbacks <- list(callback_early_stopping(patience = FLAGS$patience))
#+END_SRC

**** Long Version w/ Checks 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
model <- keras_model_sequential()

model %>%
    layer_lstm(
        units = FLAGS$n_units,
        batch_input_shape = c(FLAGS$batch_size,
                              FLAGS$n_timesteps,
                              n_features),
        dropout = FLAGS$dropout,
        recurrent_dropout = FLAGS$recurrent_dropout,
        return_sequences = TRUE,
        stateful = FLAGS$stateful
    )

if (FLAGS$stack_layers) {
    model %>%
        layer_lstm(
            units = FLAGS$n_units,
            dropout = FLAGS$dropout,
            recurrent_dropout = FLAGS$recurrent_dropout,
            return_sequences = TRUE,
            stateful = FLAGS$stateful)
}

model %>%
    time_distributed(layer_dense(units = 1))

model %>%
    compile(
        loss = FLAGS$loss,
        optimizer = optimizer,
        metrics = list("mean_squared_error")
    )

if (!FLAGS$stateful) {
    model %>%
        fit(
            x = x_train,
            y = y_train,
            validation_data = list(x_valid, y_valid),
            batch_size = FLAGS$batch_size,
            epochs = FLAGS$epochs,
            callbacks = callbacks
        )
} else {
    for (i in 1:FLAGS$epochs) {
        model %>%
            fit(
                x = x_train,
                y = y_train,
                validation_data = list(x_valid, y_valid),
                batch_size = FLAGS$batch_size,
                epochs = 1,
                shuffle = FALSE
            )        
    }
}

if (FLAGS$stateful) model %>% reset_states()
#+END_SRC

**** Short Version w/o Checks

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
model <- keras_model_sequential()

# only two layers, the LSTM and the time-distributed
model %>%
    layer_lstm(units = FLAGS$n_units,
               batch_input_shape = c(FLAGS$batch_size,
                                     FLAGS$n_timesteps,
                                     n_features),
               dropout = FLAGS$dropout,
               recurrent_dropout = FLAGS$recurrent_dropout,
               return_sequences = TRUE) %>%
    time_distributed(layer_dense(units = 1))

model %>%
    compile(loss = FLAGS$loss,
            optimizer = optimizer,
            metrics = list("mean_squared_error"))

model %>%
    fit(x = x_train,
        y = y_train,
        validation_data = list(x_valid, y_valid),
        batch_size = FLAGS$batch_size,
        epochs = FLAGS$epochs,
        callbacks = callbacks) -> history
#+END_SRC


#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Now we can check how well the model was able to capture the characteristics of the training set. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
pred_train <- model %>%
    predict(x_train, batch_size = FLAGS$batch_size) %>%
    .[, , 1]

# retransform values to original scale
pred_train <- (pred_train * scale_history + center_history)^2
compare_train <- df %>% filter(key == "training")

# build a dataframe that has actual and predicted values
for (i in 1:nrow(pred_train)) {
    varname <- paste0("pred_train", i)

    compare_train %<>%
        mutate(!!varname := c(rep(NA, FLAGS$n_timesteps + i - 1),
                              pred_train[i, ],
                              rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)))
}

# compute the average RMSE over all the sequences of predictions 
compare_train %>%
    select(-index, -value, -key) %>%
    map_dbl(., ~ rmse_vec(truth = compare_train$value,
                          estimate = .x)) %>%
    mean()
#+END_SRC

*** Visualize Predictions 

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(compare_train, aes(x = index, y = value)) + geom_line() +
    geom_line(aes(y = pred_train1), color = "cyan") +
    geom_line(aes(y = pred_train50), color = "red") +
    geom_line(aes(y = pred_train100), color = "green") +
    geom_line(aes(y = pred_train150), color = "violet") +
    geom_line(aes(y = pred_train200), color = "cyan") +
    geom_line(aes(y = pred_train250), color = "red") +
    geom_line(aes(y = pred_train300), color = "red") +
    geom_line(aes(y = pred_train350), color = "green") +
    geom_line(aes(y = pred_train400), color = "cyan") +
    geom_line(aes(y = pred_train450), color = "red") +
    geom_line(aes(y = pred_train500), color = "green") +
    geom_line(aes(y = pred_train550), color = "violet") +
    geom_line(aes(y = pred_train600), color = "cyan") +
    geom_line(aes(y = pred_train650), color = "red") +
    geom_line(aes(y = pred_train700), color = "red") +
    geom_line(aes(y = pred_train750), color = "green") +
    ggtitle("Predictions on the training set")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


Now we can check our average RMSE and fitted lines on the validation set:

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
gen_pred_test <- function(model, test_set) {
    pred_test <- model %>%
        predict(test_set, batch_size = FLAGS$batch_size) %>%
        .[, , 1]

    # retransform values to original scale
    pred_test <- (pred_test * scale_history + center_history)^2
    compare_test <- df %>% filter(key == "testing")

    # build a dataframe that has actual and predicted values
    for (i in 1:nrow(pred_test)) {
        varname <- paste0("pred_test", i)

        compare_test %<>%
            mutate(!!varname := c(rep(NA, FLAGS$n_timesteps + i - 1),
                                  pred_test[i, ],
                                  rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)))
    }

    # compute the average RMSE over all the sequences of predictions 
    compare_test %>%
        select(-index, -value, -key) %>%
        map_dbl(., ~ rmse_vec(truth = compare_test$value,
                              estimate = .x)) %>%
        mean() -> rmse_test

    return(list(compare_test, rmse_test))
}

test_results <- gen_pred_test(model, x_test)

# average RMSE 
test_results[[2]]

compare_test <- test_results[[1]]
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(compare_test, aes(x = index, y = value)) + geom_line() +
  geom_line(aes(y = pred_test1), color = "cyan") +
  geom_line(aes(y = pred_test50), color = "red") +
  geom_line(aes(y = pred_test100), color = "green") +
  geom_line(aes(y = pred_test150), color = "violet") +
  geom_line(aes(y = pred_test200), color = "cyan") +
  geom_line(aes(y = pred_test250), color = "red") +
  geom_line(aes(y = pred_test300), color = "green") +
  geom_line(aes(y = pred_test350), color = "cyan") +
  geom_line(aes(y = pred_test400), color = "red") +
  geom_line(aes(y = pred_test450), color = "green") +  
  geom_line(aes(y = pred_test500), color = "cyan") +
  geom_line(aes(y = pred_test550), color = "violet") +
  ggtitle("Predictions on test set")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Now that we've defined and run our model on a manually chosen example split, we can now revert to our overall re-sampling frame. 

** Backtesting the model on all splits 

To obtain predictions on all the splits, we move the above code into a function and apply it to all the splits.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
obtain_preds <- function(split) {
    # get CV splits
    df_trn <- analysis(split)[1:800, , drop = FALSE]
    df_val <- analysis(split)[801:1200, , drop = FALSE]
    df_tst <- assessment(split)

    # bind with time index 
    df <- bind_rows(
        df_trn %>% add_column(key = "training"),
        df_val %>% add_column(key = "validation"),
        df_tst %>% add_column(key = "testing")) %>%
        as_tbl_time(index = index)

    # preprocess data
    rec_obj <- recipe(value ~ ., df) %>%
        step_sqrt(value) %>%
        step_center(value) %>%
        step_scale(value) %>%
        prep()

    df_processed_tbl <- bake(rec_obj, df)

    # grab scale values to undo scaling later
    center_history <- rec_obj$steps[[2]]$means["value"]
    scale_history <- rec_obj$steps[[3]]$sds["value"]

    # set hyperparameters for network
    FLAGS <- flags(
        flag_boolean("stateful", FALSE),
        flag_boolean("stack_layers", FALSE),
        flag_integer("batch_size", 10),
        flag_integer("n_timesteps", 12),
        flag_integer("n_epochs", 100),
        flag_numeric("dropout", 0.2),
        flag_numeric("recurrent_dropout", 0.2),
        flag_string("loss", "logcosh"),
        flag_string("optimizer_type", "sgd"),
        flag_integer("n_units", 128),
        flag_numeric("lr", 0.003),
        flag_numeric("momentum", 0.9),
        flag_integer("patience", 10)
    )

    n_predictions <- FLAGS$n_timesteps
    n_features <- 1

    optimizer <- switch(FLAGS$optimizer_type,
                        sgd = optimizer_sgd(lr = FLAGS$lr,
                                            momentum = FLAGS$momentum))

    callbacks <- list(callback_early_stopping(patience = FLAGS$patience))

    # grab values for each CV segment
    grab_key <- function(key_val) {
        df_processed_tbl %>%
            filter(key == key_val) %>%
            select(value) %>%
            pull()
    }
    
    train_vals <- grab_key("training")
    valid_vals <- grab_key("validation")
    test_vals <- grab_key("testing")

    # build preprocessed matrices for the LSTM model 
    train_matrix <- build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)
    valid_matrix <- build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)
    test_matrix <- build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)
    
    sep_matrices <- function(data, start, end) {
        data %>% .[, start:end] %>% .[1:(nrow(.) %/% batch_size * batch_size), ] %>% reshape_X_3d()
    }
    
    x_train <- train_matrix %>% sep_matrices(1, n_timesteps)
    y_train <- train_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)

    x_valid <- valid_matrix %>% sep_matrices(1, n_timesteps)
    y_valid <- valid_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)

    x_test <- test_matrix %>% sep_matrices(1, n_timesteps)
    y_test <- test_matrix %>% sep_matrices(n_timesteps + 1, n_timesteps * 2)

    # create model
    model <- keras_model_sequential()

    # only two layers, the LSTM and the time-distributed
    model %>%
        layer_lstm(units = FLAGS$n_units,
                   batch_input_shape = c(FLAGS$batch_size,
                                         FLAGS$n_timesteps,
                                         n_features),
                   dropout = FLAGS$dropout,
                   recurrent_dropout = FLAGS$recurrent_dropout,
                   return_sequences = TRUE) %>%
        time_distributed(layer_dense(units = 1))

    model %>%
        compile(loss = FLAGS$loss,
                optimizer = optimizer,
                metrics = list("mean_squared_error"))

    model %>%
        fit(x = x_train,
            y = y_train,
            validation_data = list(x_valid, y_valid),
            batch_size = FLAGS$batch_size,
            epochs = FLAGS$n_epochs,
            callbacks = callbacks)

    
    model <- keras_model_sequential()

    model %>%
        layer_lstm(
            units = FLAGS$n_units,
            batch_input_shape = c(FLAGS$batch_size,
                                  FLAGS$n_timesteps,
                                  n_features),
            dropout = FLAGS$dropout,
            recurrent_dropout = FLAGS$recurrent_dropout,
            return_sequences = TRUE) %>%
        time_distributed(layer_dense(units = 1))

    model %>%
        compile(loss = FLAGS$loss,
                optimizer = optimizer,
                metrics = list("mean_squared_error"))

    model %>%
        fit(x = x_train,
            y = y_train,
            validation_data = list(x_valid, y_valid),
            batch_size = FLAGS$batch_size,
            epochs = FLAGS$n_epochs,
            callbacks = callbacks)

    # get predictions
    pred_train <- model %>%
        predict(x_train,
                batch_size = FLAGS$batch_size) %>%
        .[, , 1]

    # backtransform 
    pred_train <- (pred_train * scale_history + center_history)^2
    compare_train <- df %>% filter(key == "training")

    for (i in 1:nrow(pred_train)) {
        varname <- paste0("pred_train", i)

        compare_train %<>%
            mutate(!!varname := c(rep(NA, FLAGS$n_timesteps + i - 1),
                                  pred_train[i, ],
                                  rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)))
    }

    # again on the test set
    pred_test <- model %>%
        predict(test_set, batch_size = FLAGS$batch_size) %>%
        .[, , 1]

    # retransform values to original scale
    pred_test <- (pred_test * scale_history + center_history)^2
    compare_test <- df %>% filter(key == "testing")

    # build a dataframe that has actual and predicted values
    for (i in 1:nrow(pred_test)) {
        varname <- paste0("pred_test", i)
        
        compare_test %<>%
            mutate(!!varname := c(rep(NA, FLAGS$n_timesteps + i - 1),
                                  pred_test[i, ],
                                  rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)))
    }

    list(compare_train,
         compare_test)
}
#+END_SRC

Mapping the (huge) function above over all the splits yields a list of predictions 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
all_split_preds <- rolling_origin_resamples %>%
    mutate(predict = map(splits, obtain_preds))

# calculate RMSE on all the splits 
calc_rmse <- function(df) {
    df %>%
        select(-index, -value, -key) %>%
        map_dbl(., ~ rmse_vec(truth = compare_train$value,
                              estimate = .x)) %>%
        mean()
}

all_split_preds %<>% unnest(predict)
all_split_preds_train <- all_split_preds[seq(1, 11, by = 2)]
all_split_preds_test <- all_split_preds[seq(2, 12, by = 2)]

(all_split_rmses_train <- all_split_preds_train %>%
    mutate(rmse = map_dbl(predict, calc_rmse)) %>%
    select(id, rmse))

all_split_rmses_test <- all_split_preds_test %>%
    mutate(rmse = map_dbl(predict, calc_rmse)) %>%
    select(id, rmse)
#+END_SRC

