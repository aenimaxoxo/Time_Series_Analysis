* Time Series Regression Models 
:PROPERTIES:
:header-args: :session R-session :results output value table :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(magrittr)
library(fpp3)
#+END_SRC

#+RESULTS:
| x           |
|-------------|
| fable       |
| feasts      |
| fabletools  |
| tsibbledata |
| tsibble     |
| lubridate   |
| fpp3        |
| magrittr    |
| forcats     |
| stringr     |
| dplyr       |
| purrr       |
| readr       |
| tidyr       |
| tibble      |
| ggplot2     |
| tidyverse   |
| stats       |
| graphics    |
| grDevices   |
| utils       |
| datasets    |
| methods     |
| base        |

** The Linear Model 
*** Simple Linear Regression 

In the simplest case, a regression model allows for a linear relationship between the forecast variable and a single predictor variable x.


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-01 21:26:43
[[file:Time Series Regression Models/screenshot_2020-04-01_21-26-43.png]]

*** Example: US Consumption Expenditure 

#+BEGIN_SRC R :file plot.svg :results graphics file
us_change %>%
    ggplot(aes(x = Quarter)) +
    geom_line(aes(y = Consumption, color = "Consumption")) +
    geom_line(aes(y = Income, color = "Income")) +
    ylab("% Change") + xlab("Year") +
    guides(color = guide_legend(title = "Series"))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

This plot shows the percentage changes in personal consumption expenditure and personal income for the US. 

We can fit a scatter plot of consumption expenditure vs personal income change 

#+BEGIN_SRC R :file plot.svg :results graphics file
us_change %>%
    ggplot(aes(x = Income, y = Consumption)) +
    ylab("Consumption (Quarterly % Change)") +
    xlab("Income (Quarterly % Change)") +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The equation for a time series linear model is estimated in R with the TSLM function: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
us_change %>%
    model(TSLM(Consumption ~ Income)) %>%
    report() %>%
    tidy()
#+END_SRC

#+RESULTS:
| .model                     | term        | estimate | std.error | statistic |              p.value |
|----------------------------+-------------+----------+-----------+-----------+----------------------|
| TSLM(Consumption ~ Income) | (Intercept) |      0.5 |       0.1 |      10.1 | 1.62968143996723e-19 |
| TSLM(Consumption ~ Income) | Income      |      0.3 |       0.0 |       5.8 | 2.40216974893503e-08 |

We will see how TSLM fits the coefficients in Section 7.2 

The positive slope indicates a positive relationship between income and consumption. 

*** Multiple Linear Regression 

The general form of a multiple regression model is 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-01 21:35:31
[[file:Time Series Regression Models/screenshot_2020-04-01_21-35-31.png]]

The coefficients measure the marginal effects of the predictor variables. 

#+BEGIN_SRC R :file plot.svg :results graphics file
us_change %>%
    GGally::ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Assumptions 

When we use a linear regression model, we are implicitly making assumptions about the variables:

- they have mean of 0, otherwise the forecasts will be systematically biased
- they are not autocorrelated, otherwise forecasts will be inefficient
- they are unrelated to the predictor variables

It is also helpful that 
- errors are normally distributed with constant variance to produce prediction intervals
- each predictor x is not a random variable. This is because most of the data that we see is observational

** Least Squares Estimation 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-01 21:44:34
[[file:Time Series Regression Models/screenshot_2020-04-01_21-44-34.png]]

*** Example: US Consumption Expenditure 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit.consmr <- us_change %>%
    model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))

fit.consmr %>% report() %>% tidy()
#+END_SRC

#+RESULTS:
| .model | term         | estimate | std.error | statistic |              p.value |
|--------+--------------+----------+-----------+-----------+----------------------|
| tslm   | (Intercept)  |      0.3 |       0.0 |       7.3 | 5.71285129942489e-12 |
| tslm   | Income       |      0.7 |       0.0 |      18.5 |  1.6478925265192e-44 |
| tslm   | Production   |      0.0 |       0.0 |       2.0 |                  0.0 |
| tslm   | Unemployment |     -0.2 |       0.1 |      -1.8 |                  0.1 |
| tslm   | Savings      |     -0.1 |       0.0 |     -18.1 | 2.02828218945875e-43 |

*** Fitted Values 

#+BEGIN_SRC R :file plot.svg :results graphics file
fit.consmr %>%
    augment() %>%
    ggplot(aes(x = Quarter)) +
    geom_line(aes(y = Consumption, color = "Data")) +
    geom_line(aes(y = .fitted, color = "Fitted")) +
    xlab("Year") + ylab(NULL) +
    ggtitle("Percent Change in US Consumption Expenditure") +
    guides(color = guide_legend(title = NULL))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
fit.consmr %>%
    augment() %>%
    ggplot(aes(x = Consumption, y = .fitted)) +
    geom_point() +
    ylab("Fitted (Predicted Values)") +
    xlab("Data (Actual Values)") +
    ggtitle("Percent Change in US Consumption Expenditure") +
    geom_abline(intercept = 0, slope = 1) +
    geom_smooth(method = "lm", se = FALSE, lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Evaluating the Regression Model 

After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied. 

#+BEGIN_SRC R :file plot.svg :results graphics file
fit.consmr %>%
    gg_tsresiduals()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We want to look out for 

- patterns in the plot of residuals
- non normal distributions in the histogram
- lag values outside of the boundaries of our acf plot

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit.consmr %>%
    augment() %>%
    features(.resid, ljung_box, lag = 10, dof = 5)
#+END_SRC

#+RESULTS:
| .model | lb_stat | lb_pvalue |
|--------+---------+-----------|
| tslm   |    18.9 |       0.0 |

*** Residual Plots Against Predictors 

#+BEGIN_SRC R :file plot.svg :results graphics file
library(patchwork)
df <- left_join(us_change,
                residuals(fit.consmr),
                by = "Quarter")

c("Income", "Production", "Savings", "Unemployment") %>%
    map(., ~ ggplot(df, aes(x = !!sym(.x), y = .resid)) +
               geom_point() +
               ylab("Residuals")) -> p

(p[[1]] | p[[2]]) / (p[[3]] | p[[4]])                  
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Residuals Against Fitted Values 

#+BEGIN_SRC R :file plot.svg :results graphics file
fit.consmr %>%
    augment() %>%
    ggplot(aes(x = .fitted, y = .resid)) +
    geom_point() +
    labs(x = "Fitted", y = "Residuals")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Spurious Regression 

More often than not, time series data are non-stationary; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. 

Regressing non-stationary time series can lead to spurious regressions. High R^2 and high residual autocorrelation can be signs of spurious regression. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit <- aus_airpassengers %>%
    left_join(guinea_rice, by = "Year") %>%
    model(TSLM(Passengers ~ Production))

fit %>% report() %>% glance()
#+END_SRC

#+RESULTS:
| .model                        | r_squared | adj_r_squared | sigma2 | statistic |              p_value |  df | log_lik |   AIC |  AICc |   BIC |   CV | deviance | df.residual | rank |
|-------------------------------+-----------+---------------+--------+-----------+----------------------+-----+---------+-------+-------+-------+------+----------+-------------+------|
| TSLM(Passengers ~ Production) |       1.0 |           1.0 |   10.5 |     908.1 | 4.08429912736167e-29 | 2.0 |  -107.9 | 102.7 | 103.3 | 107.9 | 11.5 |    419.6 |        40.0 |  2.0 |

#+BEGIN_SRC R :file plot.svg :results graphics file
fit %>% gg_tsresiduals()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Some Useful Predictors 

*** Trend 

It is common for a time series to be trending. A linear trend can be modelled by simply using x_1,t = t as a predictor,


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-02 21:25:21
[[file:Time Series Regression Models/screenshot_2020-04-02_21-25-21.png]]

where t = 1,...,T 

A trend can be specified in the TSLM() function with the trend() special

*** Dummy Variables 

These are often used to add categorical variables to our model. We can also use it to specify things like events. TSLM() automatically handles categorical variables if a factor is specified as a variable. 

*** Seasonal Dummy Variables 

The interpretation of each of the coefficients associated with the dummy variable is that it is a measure of the effect of that category relative to the omitted category. 

*** Example: Australian Quarterly Beer Production 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
recent_production <- aus_production %>%
    filter(year(Quarter) >= 1992)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
recent_production %>%
    autoplot(Beer) +
    labs(x = "Year", y = "Megalitres")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We wish to forecast the value of future beer production. We can model this using a regression model with a linear trend and quarterly dummy variables. 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-02 21:30:09
[[file:Time Series Regression Models/screenshot_2020-04-02_21-30-09.png]]

where d_n is the quarter with the first quarter omitted.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit_beer <- recent_production %>%
    model(TSLM(Beer ~ trend() + season()))

fit_beer %>% report() %>% tidy()
#+END_SRC

#+RESULTS:
| .model                          | term          | estimate | std.error | statistic |              p.value |
|---------------------------------+---------------+----------+-----------+-----------+----------------------|
| TSLM(Beer ~ trend() + season()) | (Intercept)   |    441.8 |       3.7 |     118.3 | 2.01502266401021e-81 |
| TSLM(Beer ~ trend() + season()) | trend()       |     -0.3 |       0.1 |      -5.1 | 2.72965382379399e-06 |
| TSLM(Beer ~ trend() + season()) | season()year2 |    -34.7 |       4.0 |      -8.7 | 9.10308139144671e-13 |
| TSLM(Beer ~ trend() + season()) | season()year3 |    -17.8 |       4.0 |      -4.4 | 3.44967454483389e-05 |
| TSLM(Beer ~ trend() + season()) | season()year4 |     72.8 |       4.0 |      18.1 | 6.68309360011882e-28 |

There is an average downward trend of -0.34 megalitres per quarter. 
On average, the second quarter has production of 34 litres less than the first quarter, the third quarter has 17.8 quarters less than the first and the fourth quarter has an average of 72.8 litres more than the first quarter. 

#+BEGIN_SRC R :file plot.svg :results graphics file
fit_beer %>%
    augment() %>%
    ggplot(aes(x = Quarter)) +
    geom_line(aes(y = Beer, color = "Data")) +
    geom_line(aes(y = .fitted, color = "Fitted")) +
    labs(x = "Year", y = "Megalitres", title = "Quarterly Beer Production")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
fit_beer %>%
    augment() %>%
    ggplot(aes(x = Beer, y = .fitted,
               color = factor(quarter(Quarter)))) +
    geom_point() +
    ylab("Fitted") + xlab("Actual Values") +
    ggtitle("Quarterly Beer Production") +
    scale_color_brewer(palette = "Dark2",
                       name = "Quarter") +
    geom_abline(intercept = 0,
                slope = 1)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Intervention Variables 

It is often necessary to model interventions that may have affected the variable to be forecast. 

When the effect lasts for only one period, we use a "spike" variable. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an outlier. 

Other interventions have an immediate and permanent effect. If an intervention causes a level shift, then we use a step variable. A step variable takes value zero before the intervention and one from the time of the intervention onwards. 

*** Fourier Series 

An alternative to using seasonal dummy variables, especially for long seasonal periods, is to use Fourier terms. 

Fourier showed that a series of sine and cosine terms of the right frequencies can approximate any periodic function. 

With Fourier terms, we often need fewer predictors than with dummy variables, especially when the seasonal periods are many. This makes them useful for things like weekly data, where m =~ 52. On the flipside, they are not too useful for short seasonal periods, like quarterly data. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fourier_beer <- recent_production %>%
    model(TSLM(Beer ~ trend() + fourier(K = 2)))

report(fourier_beer) %>% tidy()
#+END_SRC

#+RESULTS:
| .model                                | term               | estimate | std.error | statistic |              p.value |
|---------------------------------------+--------------------+----------+-----------+-----------+----------------------|
| TSLM(Beer ~ trend() + fourier(K = 2)) | (Intercept)        |    446.9 |       2.9 |     155.5 | 1.39117271994188e-89 |
| TSLM(Beer ~ trend() + fourier(K = 2)) | trend()            |     -0.3 |       0.1 |      -5.1 | 2.72965382379399e-06 |
| TSLM(Beer ~ trend() + fourier(K = 2)) | fourier(K = 2)C1_4 |      8.9 |       2.0 |       4.4 | 3.44967454483378e-05 |
| TSLM(Beer ~ trend() + fourier(K = 2)) | fourier(K = 2)S1_4 |    -53.7 |       2.0 |     -26.7 | 4.10241108007111e-38 |
| TSLM(Beer ~ trend() + fourier(K = 2)) | fourier(K = 2)C2_4 |    -14.0 |       1.4 |      -9.8 | 9.25551589762455e-15 |


The K argument to fourier() determines how many pairs of sin and cos terms to include. The maximum is k/2, where m is the seasonal period. 

A regression model containing Fourier terms is often called a harmonic regression because the successive Fourier terms represent harmonics of the first two Fourier terms. 


** Selecting Predictors 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit.consmr %>% glance() %>% select(adj_r_squared,
                                   CV,
                                   AIC,
                                   AICc,
                                   BIC)
#+END_SRC

#+RESULTS:
| adj_r_squared |  CV |    AIC |   AICc |    BIC |
|---------------+-----+--------+--------+--------|
|           0.8 | 0.1 | -456.6 | -456.1 | -436.9 |

For CV, AIC, AICc, BIC we seek the term with the lowest value. For Adjusted R Squared, we seek the highest value. 

*** Adjusted R^2 

Minimizing the sum of squared errors is equivalent to maximizing R^2 and will always choose the model with the most variables. 

#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-02 21:51:01
[[file:Time Series Regression Models/screenshot_2020-04-02_21-51-01.png]]

*** Cross Validation 

This was covered in 5.8 

*** Akaike's Information Criterion 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-02 21:52:59
[[file:Time Series Regression Models/screenshot_2020-04-02_21-52-59.png]]

Where T is the number of observations used for estimation and k is the number of predictors in the model. 

The k + 2 comes from the parameters in the model: the k coefficients for the predictors, the intercept and the variance of the residuals. 

The model with the minimum value of AIC is often the best model for forecasting. For large values of T, minimizing the AIC is equivalent to minimizing the CV value. 

*** Corrected Akaike's Information Criterion

For small values of T, the AIC tends to select too many predictors. As a result, a bis-corrected version has been developed:


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-09 19:34:51
[[file:Time Series Regression Models/screenshot_2020-04-09_19-34-51.png]] 

*** Schwarz's Bayesian Information Criterion 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-09 19:35:44
[[file:Time Series Regression Models/screenshot_2020-04-09_19-35-44.png]]

The model chosen by the BIC is either the same as that chosen by the AIC or one with fewer terms. The BIC penalizes the number of parameters more heavily than the AIC. 

*** Which measure should be used? 

The authors reccommend that one of the AICc, AIC, or CV statistics should be used. If the value of T is large enough, they will all lead to the same model. 

** Forecasting with Regression 

*** Ex-ante vs. Ex-post Forecasts 

- Ex-ante forecasts are those that are made using only the information that is available in advance. These are genuine forecasts. In order to generate these forecasts, the model requires forecasts for the predictors. 

- Ex-post forecasts are those that are made using later information on the predictors. These are not genuine forecasts, but they are useful for studying the behavior of forecasting models. These forecasts can assume knowledge of the predictor values, but should not assume knowledge of the data that are to be forecast. 

A comparative evaluation of ex-ante forecasts and ex-post forecasts can help to separate out the sources of forecast uncertainty. This will show whether forecast errors have arisen due to poor forecasts of the predictor or due to a poor forecasting model. 

*** Example: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
aus_production %>%
    filter(year(Quarter) >= 1992) -> recent_production

recent_production %>%
    model(TSLM(Beer ~ trend() + season())) -> fit_beer

fit_beer %>%
    forecast() -> fc_beer 
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
fc_beer %>%
    autoplot(recent_production) +
    ggtitle("Forecasts of Beer Production Using Regression") +
    xlab("Year") + ylab("Megalitres")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Scenario Based Forecasting
 
In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# fit all the different scenarios
fit_cons_best <- us_change %>%
    model(lm = TSLM(Consumption ~ Income + Savings + Unemployment))

up_future <- new_data(us_change, 4) %>%
    mutate(Income = 1,
           Savings = 0.5,
           Unemployment = 0)

down_future <- new_data(us_change, 4) %>%
    mutate(Income = -1,
           Savings = -0.5,
           Unemployment = 0)

fc_up <- forecast(fit_cons_best,
                  new_data = up_future) %>% 
    mutate(Scenario = "Increase") %>%
    as_fable(response = Consumption,
             key = Scenario)


fc_down <- forecast(fit_cons_best,
                    new_data = down_future) %>% 
    mutate(Scenario = "Decrease") %>%
    as_fable(response = Consumption,
             key = Scenario)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
us_change %>%
    autoplot(Consumption) +
    autolayer(rbind(fc_up, fc_down)) +
    ylab("% Change in US Consumption")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** Building a Predictive Regression Model 

A major challenge is that, in order to build an ex-ante forecast, the model requires future values of each of the predictor variables. 

If scenario-based forecasting is the main interest, then these models are extremely useful. If ex-ante models are the main focus, obtaining forecasts of the predictors can be challenging (in many cases generating the forecasts for the predictors can be more challenging than forecasting directly the forecast variable without using predictors).

An alternative formulation is to use lagged variables as predictors: 

#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-09 19:57:51
[[file:Time Series Regression Models/screenshot_2020-04-09_19-57-51.png]]

Assuming that we are interested in generating an h-step ahead forecast, where h = 1,2,...

*** Prediction Intervals 

A 95% prediction interval associated with the forecast can be given by 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-09 20:00:04
[[file:Time Series Regression Models/screenshot_2020-04-09_20-00-04.png]]

Where T is the total number of observations, xbar is the mean of the observed x values, and s_x is the standard deviation of the observed x values, and sigma_hat is the standard error of the regression. 

*** Example
 
#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
fit_cons <- us_change %>%
    model(TSLM(Consumption ~ Income))

new_cons <- new_data(us_change, n = 4) %>%
    mutate(Income = mean(us_change$Income),
           Scenario = "Average Increase")

fcast_ave <- forecast(fit_cons, new_cons) %>%
    as_fable(response = Consumption,
             key = Scenario)

new_cons <- new_data(us_change, n = 4) %>%
    mutate(Income = 12,
           Scenario = "Extreme Increase")

fcast_up <- forecast(fit_cons, new_cons) %>%
    as_fable(response = Consumption,
             key = Scenario)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
us_change %>%
    autoplot(Consumption) +
    autolayer(rbind(fcast_ave,
                    fcast_up)) +
    ylab("% Change in US Consumption")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Nonlinear Regression 

The simplest way of modeling a nonlinear relationship is to transform the forecast variable y and / or the predictor variable x before estimating a regression model. While this provides a nonlinear functional form, the model is still linear in the parameters.

A log-log functional form is specified as: 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-04-09 21:42:32
[[file:Time Series Regression Models/screenshot_2020-04-09_21-42-32.png]]

where beta_1 is the average percentage change in y resulting from a 1% change in x. 

The *log-linear* form is specified by only log-transforming the forecast variable, and the linear-log form is obtained by transforming the predictor. 

*** Forecasting with a nonlinear trend 

The simplest way of fitting a nonlinear trend is using quadratic or higher order trends. This is not recommended though due to difficulties in extrapolation. 

A better approach is to use a piecewise-linear function.

*** Example 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
boston_men <- boston_marathon %>%
    filter(Event == "Men's open division") %>%
    mutate(Minutes = as.numeric(Time)/60)

fit_trends <- boston_men %>%
    model(linear = TSLM(Minutes ~ trend()),
          exponential = TSLM(log(Minutes) ~ trend()),
          piecewise = TSLM(Minutes ~ trend(knots = c(1940, 1980))))

fc_trends <- fit_trends %>% forecast(h = 10)

boston_men %>% head()
#+END_SRC

#+RESULTS:
| Event               |   Year | Champion            | Country       |     Time | Minutes |
|---------------------+--------+---------------------+---------------+----------+---------|
| Men's open division | 1897.0 | John J. McDermott   | United States | 02:55:10 |   175.2 |
| Men's open division | 1898.0 | Ronald J. MacDonald | Canada        | 02:42:00 |   162.0 |
| Men's open division | 1899.0 | Lawrence Brignolia  | United States | 02:54:38 |   174.6 |
| Men's open division | 1900.0 | John P. Caffery     | Canada        | 02:39:44 |   159.7 |
| Men's open division | 1901.0 | John P. Caffery     | Canada        | 02:29:23 |   149.4 |
| Men's open division | 1902.0 | Sammy A. Mellor     | United States | 02:43:12 |   163.2 |

#+BEGIN_SRC R :file plot.svg :results graphics file
boston_men %>%
    autoplot(Minutes) +
    geom_line(data = fitted(fit_trends),
              aes(y = .fitted, color = .model)) +
    autolayer(fc_trends, alpha = 0.5, level = 95) +
    xlab("Year") + ylab("Winning Times in Minutes") +
    ggtitle("Boston Marathon") +
    guides(color = guide_legend(title = "Model"))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]
