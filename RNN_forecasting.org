* Time Series Forecasting with Recurrent Neural Networks 
:PROPERTIES:
:header-args: :session R-session :results output value table :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(magrittr)
#+END_SRC

** Download Data 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
jena <- read_csv("jena_climate/jena_climate_2009_2016.csv")

jena %<>% janitor::clean_names()

jena %>% glimpse()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
jena %>%
    ggplot(aes(x = 1:nrow(.), y = t_deg_c)) +
    geom_line()
#+END_SRC

Here is the first 10 days of temperature data. The data is recorded every 10 minutes, giving 144 data points per day. 

#+BEGIN_SRC R :file plot.svg :results graphics file
jena %>%
    .[1:1440, ] %>%
    ggplot(aes(x = 1:1440, y = t_deg_c)) +
    geom_line()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

lookback: 1440 observations will go back 10 days
steps: 6 observations will be sampled at one data point per hour 
delay: 144 targets will be 24 hours in the future 

We must:
- preprocess the data to a format that the neural network can ingest. We also need to normalize the data since it is all on different scales 
- write a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future.


** Data Preprocessing 

First we will convert the R data frame into a matrix of floating point values. The first column included a text timestamp so we can discard it. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
jena <- data.matrix(jena[, -1])

train_data <- jena[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)

jena <- scale(jena, center = mean, scale = std)
#+END_SRC

** Define the Generator 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
    if (is.null(max_index)) max_index <- nrow(data) - delay - 1
    i <- min_index

    function() {
        if (shuffle) {
            rows <- sample(c(min_index + lookback):max_index,
                           size = batch_size)
        } else {
            if (i + batch_size >= max_index) i <<- min_index + lookback
            rows <- c(i:min(i + batch_size - 1,
                            max_index))
            i <<- i + length(rows)
        }

        samples <- array(0, dim = c(length(rows),
                                    lookback / step,
                                    dim(data)[[-1]]))

        targets <- array(0, dim = c(length(rows)))

        for (j in 1:length(rows)) {
            indices <- seq(rows[[j]] - lookback,
                           rows[[j]] - 1,
                           length.out = dim(samples)[[2]])

            samples[j, , ] <- data[indices, ]
            targets[[j]] <- data[rows[[j]] + delay, 2]
        }
        list(samples, targets)
    }
}
#+END_SRC

Now we can use the abstract generator function to instantiate 3 generators: training, validation, testing 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

train_gen <- generator(
    data = jena,
    lookback = lookback,
    delay = delay,
    min_index = 1,
    max_index = 200000,
    shuffle = TRUE,
    step = step,
    batch_size = batch_size
)

validation_gen <- generator(
    data = jena,
    lookback = lookback,
    delay = delay,
    min_index = 200001,
    max_index = 300000,
    shuffle = TRUE,
    step = step,
    batch_size = batch_size
)

test_gen <- generator(
    data = jena,
    lookback = lookback,
    delay = delay,
    min_index = 300001,
    max_index = NULL,
    shuffle = TRUE,
    step = step,
    batch_size = batch_size
)

# how many steps to draw from val_gen in order to see the entire validation set 
val_steps <- (300000 - 200001 - lookback) / batch_size 
test_steps <- (nrow(jena) - 300001 - lookback) / batch_size
#+END_SRC

** A common sense, non-machine learning baseline 

We could predict that the temperature 24 hours from now will be equal to the temperature right now.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(keras)

eval_naive_method <- function() {
    batch_maes <- c()

    for (step in 1:val_steps) {
        c(samples, targets) %<-% validation_gen()
        preds <- samples[, dim(samples)[[2]], 2]
        mae <- mean(abs(preds - targets))
        batch_maes <- c(batch_maes, mae)
    }

    print(mean(batch_maes))
}

# this will be scaled, so multiply by the temp std dev 
(eval_naive_method() * std[[2]])
#+END_SRC

** A Basic Machine Learning Approach 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
keras_model_sequential() %>%
    layer_flatten(input_shape = c(lookback / step,
                                  dim(data)[-1])) %>%
    layer_dense(units = 32,
                activation = "relu") %>%
    layer_dense(units = 1) -> model 

model %>%
    compile(optimizer = optimizer_rmsprop(),
            loss = "mae")

model %>%
    fit_generator(train_gen,
                  steps_per_epoch = 500,
                  epochs = 20,
                  validation_data = validation_gen,
                  validation_steps = val_steps) -> history
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC

** A First Recurrent Baseline 

The previous approach flattened the time series, which removed the notion of time from the input data. Instead, we should look at it as a sequence where causality and order matter. 

We will start with a gated recurrent unit. This is the same principle as the lstm layer, but it is more streamlined and cheaper to run. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
keras_model_sequential() %>%
    layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>%
    layer_dense(units = 1) -> model

model %>% compile(optimizer = optimizer_rmsprop(),
                  loss = "mae")

model %>% fit_generator(train_gen,
                        steps_per_epoch = 500,
                        epochs = 20,
                        validation_data = validation_gen,
                        validation_steps = val_steps) -> history
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC

** Using Recurrent Dropout to Fight Overfitting 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
keras_model_sequential() %>%
    layer_gru(units = 32,
              dropout = 0.2,
              recurrent_dropout = 0.2,
              input_shape = list(NULL, dim(data)[[-1]])) %>%
    layer_dense(units = 1) %>%
    compile(optimizer = optimizer_rmsprop(),
            loss = "mae") %>%
    fit_generator(train_gen,
                  steps_per_epoch = 500,
                  epochs = 40,
                  validation_data = validation_gen,
                  validation_steps = val_steps) -> history
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC

** Stacking Recurrent Layers 

To stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output. This is done by specifying return_sequences = TRUE. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
keras_model_sequential() %>%
    layer_gru(units = 32,
              dropout = 0.1,
              recurrent_dropout = 0.5,
              return_sequences = TRUE,
              input_shape = list(NULL, dim(data)[[-1]])) %>%
    layer_gru(units = 64,
              activation = "relu",
              dropout = 0.1,
              recurrent_dropout = 0.5) %>%
    layer_dense(units = 1) %>%
    compile(optimizer = optimizer_rmsprop(),
            loss = "mae") %>%
    fit_generator(train_gen,
                  steps_per_epoch = 500,
                  epochs = 40,
                  validaton_data = validation_gen,
                  validation_steps = val_steps) -> history
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC

** Using Bidirectional Recurrent Neural Networks 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
keras_model_sequential() %>%
    bidirectional(layer_gru(units = 32),
                  input_shape = list(NULL, dim(data)[[-1]])) %>%
    layer_dense(units = 1) %>%
    compile(optimizer = optimizer_rmsprop(),
            loss = "mae") %>%
    fit_generator(train_gen,
                  steps_per_epoch = 500,
                  epochs = 40,
                  validation_data = validation_gen,
                  validation_steps = val_steps) -> history
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
plot(history)
#+END_SRC
